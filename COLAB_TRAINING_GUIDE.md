# Mathematical FFN-v2 Training in Google Colab

## Overview
This guide provides step-by-step instructions for running the mathematically optimized FFN-v2 training script in Google Colab for neuron tracing.

## ðŸš€ Quick Start

### Step 1: Open Google Colab & Configure GPU
1.  Go to [Google Colab](https://colab.research.google.com/) and create a new notebook.
2.  Change the runtime to use a premium GPU:
    -   Go to `Runtime` â†’ `Change runtime type`.
    -   Set `Runtime type` to `Python 3`.
    -   For **Hardware accelerator**, select **A100 GPU**. This is the fastest option available with Colab Pro and is highly recommended for this model.
    -   Optionally, enable the **High-RAM** setting if you plan to work with larger data volumes.
    -   Click **Save**.

### Step 2: Copy and Run the Training Script
1. Copy the entire contents of `colab_mathematical_training_final.py`
2. Paste it into a Colab cell
3. Run the cell (Shift + Enter)

### Step 3: Monitor Training Progress
The script will automatically:
- Install required packages
- Set up GPU environment (avoiding Triton conflicts)
- Generate realistic neuron training data
- Train the mathematical FFN-v2 model
- Plot training curves
- Save the best and final models

## Expected Output

### Training Progress
```
Setting up Mathematical FFN-v2 Training Environment...
GPU available and enabled
Using device: cuda
PyTorch version: 2.x.x

Mathematical FFN-v2 Training for Neuron Tracing
============================================================
Configuration:
   input_channels: 1
   output_channels: 1
   hidden_channels: 64
   depth: 3
   learning_rate: 0.001
   batch_size: 4
   num_epochs: 50
   train_samples: 100
   val_samples: 20
   volume_size: (64, 64, 64)

Initializing components...
Generating training data...
Generated 100 training samples and 20 validation samples

Starting training...
Starting Mathematical FFN-v2 Training for 50 epochs...
Training samples: 100
Validation samples: 20

Epoch   1/50 | Train Loss: 0.8234 | Val Loss: 0.7891 | LR: 1.00e-03 | Time: 12.3s
Epoch   2/50 | Train Loss: 0.7123 | Val Loss: 0.6987 | LR: 9.95e-04 | Time: 11.8s
...
New best model saved! Val Loss: 0.2345
...
Training completed in 589.2 seconds
```

### Generated Files
- `best_mathematical_ffn_v2.pt` - Best model based on validation loss
- `final_mathematical_ffn_v2.pt` - Final model after training
- `mathematical_training_curves.png` - Training loss and learning rate curves

## Downloading Results

After training completes, download the results:

```python
from google.colab import files

# Download the best model
files.download('best_mathematical_ffn_v2.pt')

# Download training curves
files.download('mathematical_training_curves.png')

# Download final model (optional)
files.download('final_mathematical_ffn_v2.pt')
```

## Configuration Options

You can modify the training configuration by editing the `config` dictionary in the script:

```python
config = {
    'input_channels': 1,        # Input channels (1 for grayscale)
    'output_channels': 1,       # Output channels (1 for binary segmentation)
    'hidden_channels': 64,      # Hidden layer channels (64-128 recommended)
    'depth': 3,                 # Network depth (3-5 recommended)
    'learning_rate': 1e-3,      # Learning rate (1e-3 to 1e-4)
    'batch_size': 4,            # Batch size (adjust based on GPU memory)
    'num_epochs': 50,           # Training epochs (50-100 recommended)
    'train_samples': 100,       # Training samples (100-500 recommended)
    'val_samples': 20,          # Validation samples (20-50 recommended)
    'volume_size': (64, 64, 64) # Volume size (64Â³ to 128Â³ recommended)
}
```

## Mathematical Optimizations Included

### Architecture Optimizations
- **Xavier/Glorot Weight Initialization**: Optimal gradient flow
- **Residual Connections**: Improved gradient propagation
- **Batch Normalization**: Internal covariate shift reduction
- **Dropout Regularization**: Stochastic regularization

### Training Optimizations
- **AdamW Optimizer**: Advanced adaptive learning with weight decay
- **Cosine Annealing Scheduler**: Learning rate scheduling with warm restarts
- **Gradient Clipping**: Numerical stability
- **Early Stopping**: Prevent overfitting

### Loss Function Optimizations
- **Combined Loss**: BCE + Dice + Focal loss
- **Mathematical Weighting**: Optimal loss combination
- **Class Imbalance Handling**: Focal loss for rare classes

### Data Generation Optimizations
- **Realistic Neuron Structures**: Mathematical morphology modeling
- **Dendritic Branching**: Biological accuracy
- **Noise Addition**: Training robustness

## Troubleshooting

### GPU Memory Issues
If you encounter GPU memory errors:
```python
# Reduce batch size
config['batch_size'] = 2

# Reduce volume size
config['volume_size'] = (32, 32, 32)

# Reduce hidden channels
config['hidden_channels'] = 32
```

### Triton Library Conflicts
The script automatically handles Triton conflicts by:
1. Disabling CUDA initially
2. Importing PyTorch on CPU
3. Re-enabling GPU after import

### Training Not Converging
If training loss doesn't decrease:
1. Increase learning rate: `config['learning_rate'] = 5e-3`
2. Increase training samples: `config['train_samples'] = 200`
3. Increase epochs: `config['num_epochs'] = 100`

## ðŸ“ˆ Performance Expectations

### Typical Training Times
- **A100 GPU (Colab Pro)**:
  - **50 epochs**: ~4-6 minutes
  - **100 epochs**: ~8-12 minutes
  - **200 epochs**: ~15-25 minutes
- **Tesla T4 (Standard Colab GPU)**:
  - **50 epochs**: ~10-15 minutes
  - **100 epochs**: ~20-30 minutes
  - **200 epochs**: ~40-60 minutes

Using an A100 GPU can accelerate training by 2-3x compared to the standard T4 GPU.

### Expected Loss Values
- **Initial loss**: 0.7-0.9
- **Final loss**: 0.1-0.3
- **Best validation loss**: 0.05-0.2

### Model Performance
- **Training accuracy**: 85-95%
- **Validation accuracy**: 80-90%
- **Segmentation quality**: High fidelity neuron tracing

## Advanced Usage

### Custom Data Integration
To use your own H01 dataset:
```python
# Replace data generation with your data
train_inputs = torch.load('your_h01_data.pt')
train_targets = torch.load('your_h01_labels.pt')
```

### Hyperparameter Tuning
For systematic hyperparameter optimization:
```python
# Grid search example
learning_rates = [1e-4, 5e-4, 1e-3, 5e-3]
hidden_channels = [32, 64, 128]

for lr in learning_rates:
    for hc in hidden_channels:
        config['learning_rate'] = lr
        config['hidden_channels'] = hc
        # Run training...
```

## Mathematical Background

The training script incorporates insights from:
- **Convex Optimization** (Boyd & Vandenberghe)
- **Matrix Analysis** (Bellman)
- **Neural Network Optimization** (various papers)
- **Deep Learning Optimization** (Adam, AdamW papers)

## Success Indicators

Training is successful when you see:
- Steady decrease in training loss
- Validation loss following training loss
- Learning rate schedule working properly
- Early stopping not triggered too early
- Final validation loss < 0.3

## Support

If you encounter issues:
1. Check GPU runtime is enabled
2. Verify all packages installed correctly
3. Monitor GPU memory usage
4. Check training curves for convergence
5. Ensure sufficient training samples

---

**Happy Training!** 