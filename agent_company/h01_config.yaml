# H01 Dataset Configuration for Agentic Tracer
# Based on Google FFN patterns and H01 release data
# https://h01-release.storage.googleapis.com/data.html

# Data source configuration
data_source:
  type: "h01"
  cloudvolume:
    cloud_path: "gs://h01-release/data/20210601/4nm_raw"
    mip: 0  # Base resolution
    bounds: [[0, 0, 0], [1000, 1000, 1000]]  # Default bounds

# H01 specific regions (based on the dataset structure)
h01_regions:
  test_region:
    bounds: [[1000, 1000, 1000], [2000, 2000, 2000]]
    description: "Small test region for validation"
    size_gb: 0.001
  
  validation_region:
    bounds: [[5000, 5000, 5000], [10000, 10000, 10000]]
    description: "Medium validation region"
    size_gb: 0.125
  
  training_region:
    bounds: [[10000, 10000, 10000], [20000, 20000, 20000]]
    description: "Large training region"
    size_gb: 1.0
  
  production_region:
    bounds: [[20000, 20000, 20000], [40000, 40000, 40000]]
    description: "Production-scale region"
    size_gb: 8.0

# Data access configuration
data_access:
  caching:
    enabled: true
    cache_dir: "./h01_cache"
    max_size_gb: 10.0
  
  chunking:
    default_chunk_size: [64, 64, 64]
    overlap: [8, 8, 8]
  
  retry:
    max_attempts: 3
    backoff_factor: 2.0
    timeout_seconds: 30

# Model configuration for H01
model:
  name: "MathematicalFFNv2"
  input_shape: [64, 64, 64]
  output_channels: 1
  
  # H01-specific model parameters
  fov_size: [33, 33, 33]
  deltas: [8, 8, 8]
  depth: 12
  
  # Training parameters optimized for H01
  batch_size: 4
  learning_rate: 0.001
  num_epochs: 100
  
  # H01 voxel size: 4nm x 4nm x 33nm
  voxel_size: [4, 4, 33]

# Training configuration
training:
  # Coordinate sampling (similar to Google FFN)
  coordinate_sampling:
    method: "uniform"
    margin: [24, 24, 24]  # Based on fov_size + deltas
    min_size: 10000
  
  # Data augmentation for H01
  augmentation:
    enabled: true
    rotation: true
    flip: true
    noise: true
    intensity_scale: [0.8, 1.2]
  
  # Validation settings
  validation:
    frequency: 100  # Validate every 100 steps
    region: "validation_region"
  
  # Checkpointing
  checkpointing:
    frequency: 500  # Save every 500 steps
    keep_last: 5
    save_dir: "./h01_checkpoints"

# Inference configuration
inference:
  # Flood-filling parameters
  flood_filling:
    threshold: 0.5
    min_size: 1000
    max_size: 1000000
  
  # Seed point selection
  seed_selection:
    method: "random"
    num_seeds: 100
    min_distance: 50
  
  # Output format
  output:
    format: "npz"  # NumPy compressed format
    include_probabilities: true
    compression: "gzip"

# Monitoring and logging
monitoring:
  enabled: true
  log_level: "INFO"
  
  # Metrics to track
  metrics:
    - "loss"
    - "accuracy"
    - "dice_score"
    - "memory_usage"
    - "data_load_time"
  
  # Visualization
  visualization:
    enabled: true
    frequency: 50
    save_dir: "./h01_visualizations"

# Robustness testing
robustness:
  enabled: true
  
  # Test scenarios
  test_scenarios:
    - name: "small_region"
      region: "test_region"
      description: "Test with small region"
    
    - name: "medium_region"
      region: "validation_region"
      description: "Test with medium region"
    
    - name: "large_region"
      region: "training_region"
      description: "Test with large region"
  
  # Performance benchmarks
  benchmarks:
    memory_limit_gb: 16.0
    time_limit_seconds: 3600
    accuracy_threshold: 0.8

# Environment-specific settings
environment:
  colab:
    # Colab-specific optimizations
    use_mixed_precision: true
    memory_growth: true
    data_parallel: false
    
  local:
    # Local development settings
    use_mixed_precision: false
    memory_growth: false
    data_parallel: true

# Processing Configuration (optimized for H01 data)
processing:
  # Smaller chunks for the high-resolution H01 data
  chunk_size: [128, 128, 128]  # 4nm resolution chunks
  overlap: [16, 16, 16]        # Overlap for seamless stitching
  batch_size: 2                # Conservative batch size for large data
  num_processes: 4             # Parallel processing
  num_threads: 8               # Thread pool size
  max_memory_gb: 32            # H01 data is memory-intensive
  cache_size: "8GB"            # Large cache for cloud data

# Proofreading Configuration (optimized for H01)
proofreading:
  error_detection:
    use_topology: true
    use_morphology: true
    use_consistency: true
    use_boundary: true
    use_connectivity: true
    min_component_size: 1000    # H01 has larger structures
    connectivity_threshold: 10  # Larger threshold for H01 scale
  
  error_correction:
    use_morphological: true
    use_topological: true
    use_interpolation: true
    use_smoothing: true
    use_reconstruction: true
    morphology_kernel_size: 5   # Larger kernel for H01 resolution
    smoothing_sigma: 1.5

# Output Configuration
output:
  base_path: "./h01_output"
  formats:
    - "numpy"
    - "zarr"  # Better for large datasets
  compression:
    algorithm: "blosc"
    level: 6
  save_metadata: true
  save_quality_metrics: true
  save_processing_logs: true

# Telemetry and Monitoring
telemetry:
  prometheus:
    enabled: true
    port: 8000
    host: "localhost"
  
  system_monitoring:
    enabled: true
    interval_seconds: 5
    metrics:
      - "cpu_usage"
      - "memory_usage"
      - "gpu_usage"
      - "disk_io"
      - "network_io"
  
  logging:
    level: "INFO"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    file: "h01_production.log"
    max_size_mb: 100
    backup_count: 5

# Performance Tuning for H01
performance:
  gpu:
    enabled: true
    memory_fraction: 0.9        # Use most of GPU memory
    allow_growth: true
  
  cpu:
    num_workers: 4
    pin_memory: true
  
  io:
    prefetch_factor: 4          # Higher prefetch for cloud data
    persistent_workers: true
    pin_memory: true

# H01-specific validation
validation:
  quality_thresholds:
    min_confidence: 0.8         # Higher confidence for H01
    max_uncertainty: 0.2        # Lower uncertainty threshold
    min_component_size: 500     # Larger minimum for H01 scale
  
  checks:
    - "segmentation_quality"
    - "proofreading_effectiveness"
    - "continual_learning_progress"
    - "system_performance"
    - "data_consistency"

# Data access settings
data_access:
  # Authentication (if needed)
  authentication:
    use_default_credentials: true
    service_account_key: null  # Set if using service account
  
  # Caching settings
  caching:
    enabled: true
    cache_dir: './h01_cache'
    max_cache_size: "50GB"
    cache_ttl_hours: 24
  
  # Network settings
  network:
    timeout_seconds: 300
    retry_attempts: 3
    chunk_size_bytes: 1048576  # 1MB chunks for download 