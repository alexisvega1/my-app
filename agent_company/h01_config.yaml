# H01 Dataset Configuration for Agentic Tracer
# ===========================================
# Based on https://h01-release.storage.googleapis.com/data.html

# Data Source Configuration
data_source:
  type: "cloudvolume"
  cloudvolume:
    # 4nm EM data - the raw electron microscopy data
    cloud_path: "gs://h01-release/data/20210601/4nm_raw"
    mip: 0  # Use highest resolution
    bounds: [[0, 0, 0], [1000, 1000, 1000]]  # Start with a small region for testing
    # Full bounds: [[0, 0, 0], [100000, 100000, 100000]]  # Full dataset is massive
    
    # Alternative data sources available:
    # - "gs://h01-release/data/20210601/masking"  # Masking model
    # - "gs://h01-release/data/20210601/c2/"      # C2 segmentation
    # - "gs://h01-release/data/20210601/c3/"      # C3 segmentation
    # - "gs://h01-release/data/20210601/proofread_104"  # 104 proofread cells

# Processing Configuration (optimized for H01 data)
processing:
  # Smaller chunks for the high-resolution H01 data
  chunk_size: [128, 128, 128]  # 4nm resolution chunks
  overlap: [16, 16, 16]        # Overlap for seamless stitching
  batch_size: 2                # Conservative batch size for large data
  num_processes: 4             # Parallel processing
  num_threads: 8               # Thread pool size
  max_memory_gb: 32            # H01 data is memory-intensive
  cache_size: "8GB"            # Large cache for cloud data

# Model Configuration
model:
  ffn_v2:
    model_path: "quick_ffn_v2_model.pt"
    input_channels: 1
    hidden_channels: [32, 64, 128, 256]  # Larger model for H01 complexity
    output_channels: 1
    use_attention: true
    dropout_rate: 0.1
  
  continual_learning:
    enabled: true
    adaptation_strategy: "low_rank"
    rank: 32                    # Higher rank for complex H01 data
    alpha: 64.0                 # Higher alpha for better adaptation
    learning_rate: 5e-5         # Lower learning rate for stability
    batch_size: 16
    num_epochs: 5

# Proofreading Configuration (optimized for H01)
proofreading:
  error_detection:
    use_topology: true
    use_morphology: true
    use_consistency: true
    use_boundary: true
    use_connectivity: true
    min_component_size: 1000    # H01 has larger structures
    connectivity_threshold: 10  # Larger threshold for H01 scale
  
  error_correction:
    use_morphological: true
    use_topological: true
    use_interpolation: true
    use_smoothing: true
    use_reconstruction: true
    morphology_kernel_size: 5   # Larger kernel for H01 resolution
    smoothing_sigma: 1.5

# Output Configuration
output:
  base_path: "./h01_output"
  formats:
    - "numpy"
    - "zarr"  # Better for large datasets
  compression:
    algorithm: "blosc"
    level: 6
  save_metadata: true
  save_quality_metrics: true
  save_processing_logs: true

# Telemetry and Monitoring
telemetry:
  prometheus:
    enabled: true
    port: 8000
    host: "localhost"
  
  system_monitoring:
    enabled: true
    interval_seconds: 5
    metrics:
      - "cpu_usage"
      - "memory_usage"
      - "gpu_usage"
      - "disk_io"
      - "network_io"
  
  logging:
    level: "INFO"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    file: "h01_production.log"
    max_size_mb: 100
    backup_count: 5

# Performance Tuning for H01
performance:
  gpu:
    enabled: true
    memory_fraction: 0.9        # Use most of GPU memory
    allow_growth: true
  
  cpu:
    num_workers: 4
    pin_memory: true
  
  io:
    prefetch_factor: 4          # Higher prefetch for cloud data
    persistent_workers: true
    pin_memory: true

# H01-specific validation
validation:
  quality_thresholds:
    min_confidence: 0.8         # Higher confidence for H01
    max_uncertainty: 0.2        # Lower uncertainty threshold
    min_component_size: 500     # Larger minimum for H01 scale
  
  checks:
    - "segmentation_quality"
    - "proofreading_effectiveness"
    - "continual_learning_progress"
    - "system_performance"
    - "data_consistency"

# H01 dataset regions for testing
h01_regions:
  # Small test region (1GB)
  test_region:
    bounds: [[0, 0, 0], [1000, 1000, 1000]]
    description: "Small test region for validation"
  
  # Medium region (10GB)
  medium_region:
    bounds: [[0, 0, 0], [3000, 3000, 3000]]
    description: "Medium region for processing"
  
  # Large region (100GB+)
  large_region:
    bounds: [[0, 0, 0], [10000, 10000, 10000]]
    description: "Large region for production processing"
  
  # Specific regions of interest
  regions_of_interest:
    - name: "cortical_layer_1"
      bounds: [[5000, 5000, 5000], [6000, 6000, 6000]]
      description: "Cortical layer 1 region"
    
    - name: "blood_vessel_region"
      bounds: [[7000, 7000, 7000], [8000, 8000, 8000]]
      description: "Region with blood vessels"
    
    - name: "synapse_dense_region"
      bounds: [[9000, 9000, 9000], [10000, 10000, 10000]]
      description: "Synapse-dense region"

# Data access settings
data_access:
  # Authentication (if needed)
  authentication:
    use_default_credentials: true
    service_account_key: null  # Set if using service account
  
  # Caching settings
  caching:
    enabled: true
    cache_dir: "./h01_cache"
    max_cache_size: "50GB"
    cache_ttl_hours: 24
  
  # Network settings
  network:
    timeout_seconds: 300
    retry_attempts: 3
    chunk_size_bytes: 1048576  # 1MB chunks for download 