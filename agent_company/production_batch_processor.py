#!/usr/bin/env python3
"""
Production Batch Processor for H01 Connectomics
==============================================
Large-scale batch processing system for H01 connectomics data with:
- Parallel processing of multiple regions
- Cloud deployment support
- Advanced analysis pipelines
- Integration with external tools
"""

import numpy as np
import os
import json
import logging
import time
import multiprocessing as mp
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import psutil
import threading
from datetime import datetime
import queue
import subprocess
import shutil

# Import our pipeline components
from neuron_tracer_3d import NeuronTracer3D
from visualization import H01Visualizer
from extract_h01_brain_regions import H01RegionExtractor
from advanced_analysis import AdvancedAnalysisPipeline

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class BatchConfig:
    """Configuration for batch processing."""
    # Processing parameters
    regions: List[str] = None
    region_sizes: List[str] = None  # ["small", "medium", "large"]
    chunk_size: Tuple[int, int, int] = (512, 512, 256)
    overlap: Tuple[int, int, int] = (64, 64, 32)
    
    # Parallel processing
    max_workers: int = mp.cpu_count()
    max_concurrent_regions: int = 4
    max_memory_per_worker_gb: float = 8.0
    
    # Resource limits
    total_memory_limit_gb: float = 64.0
    max_cpu_percent: float = 90.0
    
    # Output settings
    output_base_dir: str = "h01_batch_results"
    save_intermediate: bool = True
    compression: str = "gzip"
    
    # Advanced analysis
    enable_synapse_detection: bool = True
    enable_circuit_mapping: bool = True
    enable_swc_export: bool = True
    enable_neurolucida_export: bool = True
    
    # Cloud deployment
    cloud_deployment: bool = False
    cloud_config: Dict = None

class ExternalToolIntegrator:
    """Integrate with external connectomics tools."""
    
    def __init__(self, config: BatchConfig):
        self.config = config
        
    def export_to_swc(self, traces: Dict, output_path: str) -> bool:
        """Export neuron traces to SWC format for Vaa3D/Neurolucida."""
        logger.info(f"Exporting traces to SWC format: {output_path}")
        
        try:
            with open(output_path, 'w') as f:
                # Write header
                f.write("# SWC file generated by H01 Production Pipeline\n")
                f.write("# Format: id type x y z radius parent_id\n")
                
                for neuron_id, neuron in traces.items():
                    # Convert neuron coordinates to SWC format
                    coords = neuron.coordinates
                    
                    for i, coord in enumerate(coords):
                        # SWC format: id type x y z radius parent_id
                        # type 2 = axon, type 3 = dendrite, type 1 = soma
                        node_type = 2 if i == 0 else 3  # Simplified typing
                        
                        # Calculate radius (simplified)
                        radius = max(1.0, np.sqrt(neuron.volume / len(coords)) / 10)
                        
                        # Parent ID (simplified tree structure)
                        parent_id = -1 if i == 0 else i
                        
                        f.write(f"{i+1} {node_type} {coord[2]} {coord[1]} {coord[0]} {radius:.2f} {parent_id}\n")
            
            logger.info(f"✓ Exported {len(traces)} neurons to SWC")
            return True
            
        except Exception as e:
            logger.error(f"✗ Failed to export SWC: {e}")
            return False
    
    def export_to_neurolucida(self, traces: Dict, output_path: str) -> bool:
        """Export to Neurolucida format."""
        logger.info(f"Exporting to Neurolucida format: {output_path}")
        
        try:
            with open(output_path, 'w') as f:
                f.write("(ImageFile H01_Connectomics)\n")
                f.write("(ImageCoords 0 0 0 0 0 0)\n")
                f.write("(Sections\n")
                
                for neuron_id, neuron in traces.items():
                    f.write(f"  (Cell {neuron_id}\n")
                    
                    # Write neuron as a series of points
                    coords = neuron.coordinates
                    for coord in coords:
                        f.write(f"    (Point {coord[2]} {coord[1]} {coord[0]})\n")
                    
                    f.write("  )\n")
                
                f.write(")\n")
            
            logger.info(f"✓ Exported to Neurolucida format")
            return True
            
        except Exception as e:
            logger.error(f"✗ Failed to export Neurolucida: {e}")
            return False

class ProductionBatchProcessor:
    """Production-scale batch processor for H01 data."""
    
    def __init__(self, config: BatchConfig):
        self.config = config
        self.output_dir = Path(config.output_base_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize components
        self.extractor = H01RegionExtractor()
        self.advanced_pipeline = AdvancedAnalysisPipeline()
        self.integrator = ExternalToolIntegrator(config)
        
        # Processing state
        self.processed_regions = []
        self.failed_regions = []
        self.processing_stats = {}
        
        logger.info(f"Production batch processor initialized")
        logger.info(f"Output directory: {self.output_dir}")
        logger.info(f"Max workers: {config.max_workers}")
        logger.info(f"Max concurrent regions: {config.max_concurrent_regions}")
    
    def process_region_batch(self, region_name: str, size: str) -> Dict[str, Any]:
        """Process a single region with advanced analysis."""
        start_time = time.time()
        region_output_dir = self.output_dir / f"{region_name}_{size}"
        region_output_dir.mkdir(exist_ok=True)
        
        try:
            logger.info(f"Processing region: {region_name} ({size})")
            
            # Step 1: Extract region
            logger.info(f"  Step 1: Extracting {region_name}...")
            region_data = self.extractor.extract_region(region_name, size)
            
            if region_data is None:
                raise RuntimeError(f"Failed to extract region {region_name}")
            
            # Save raw data
            raw_file = region_output_dir / "raw_data.npy"
            np.save(raw_file, region_data)
            logger.info(f"  ✓ Saved raw data: {raw_file}")
            
            # Step 2: Create segmentation
            logger.info(f"  Step 2: Creating segmentation...")
            segmentation = self._create_segmentation(region_data)
            
            seg_file = region_output_dir / "segmentation.npy"
            np.save(seg_file, segmentation)
            logger.info(f"  ✓ Saved segmentation: {seg_file}")
            
            # Step 3: Neuron tracing
            logger.info(f"  Step 3: Tracing neurons...")
            tracer = NeuronTracer3D(segmentation_data=segmentation)
            tracer.analyze_connectivity(distance_threshold=10.0)
            traces_file = region_output_dir / "traces.json"
            tracer.export_traces(str(traces_file))
            logger.info(f"  ✓ Saved traces: {traces_file}")
            
            # Step 4: Advanced analysis (integrated)
            logger.info(f"  Step 4: Advanced analysis pipeline...")
            # Use the advanced analysis pipeline
            # Detect synapses using the simple method for compatibility
            synapse_results = []
            if self.config.enable_synapse_detection:
                synapse_results = []
                for neuron_id, neuron in tracer.traced_neurons.items():
                    coords = neuron.coordinates
                    for coord in coords[::10]:
                        z, y, x = coord
                        if (0 <= z < region_data.shape[0] and 0 <= y < region_data.shape[1] and 0 <= x < region_data.shape[2]):
                            local_region = region_data[
                                max(0, z-2):min(region_data.shape[0], z+3),
                                max(0, y-2):min(region_data.shape[1], y+3),
                                max(0, x-2):min(region_data.shape[2], x+3)
                            ]
                            if local_region.size > 0 and np.mean(local_region) > 200:
                                synapse_results.append({
                                    'id': len(synapse_results),
                                    'neuron_id': neuron_id,
                                    'coordinates': [int(z), int(y), int(x)],
                                    'intensity': float(np.mean(local_region)),
                                    'confidence': 0.8
                                })
            # Build connectivity graph for motif analysis
            connectivity_graph = {}
            for neuron_id, neuron in tracer.traced_neurons.items():
                connectivity_graph[neuron_id] = {
                    'connections': neuron.connectivity,
                    'synapses': [s for s in synapse_results if s['neuron_id'] == neuron_id]
                }
            # Run advanced analysis pipeline
            advanced_results = self.advanced_pipeline.run_complete_analysis(
                region_data, tracer.traced_neurons, synapse_results, connectivity_graph
            )
            # Save advanced analysis report
            adv_report_file = region_output_dir / "advanced_analysis_report.json"
            with open(adv_report_file, 'w') as f:
                json.dump(advanced_results['comprehensive_report'], f, indent=2)
            logger.info(f"  ✓ Saved advanced analysis report: {adv_report_file}")
            
            # Step 5: External tool exports
            if self.config.enable_swc_export:
                logger.info(f"  Step 5a: SWC export...")
                swc_file = region_output_dir / "neurons.swc"
                self.integrator.export_to_swc(tracer.traced_neurons, str(swc_file))
            
            if self.config.enable_neurolucida_export:
                logger.info(f"  Step 5b: Neurolucida export...")
                nl_file = region_output_dir / "neurons.asc"
                self.integrator.export_to_neurolucida(tracer.traced_neurons, str(nl_file))
            
            # Step 6: Visualization
            logger.info(f"  Step 6: Creating visualizations...")
            self._create_advanced_visualizations(region_data, segmentation, tracer, 
                                               advanced_results, region_output_dir)
            
            # Step 7: Create comprehensive report
            logger.info(f"  Step 7: Generating report...")
            self._create_advanced_report(region_name, size, region_data, segmentation, 
                                       tracer, advanced_results, region_output_dir)
            
            processing_time = time.time() - start_time
            
            result = {
                'region_name': region_name,
                'size': size,
                'status': 'success',
                'processing_time': processing_time,
                'data_shape': region_data.shape,
                'num_neurons': len(tracer.traced_neurons),
                'num_synapses': len(advanced_results.get('synapses', {}).get('synapses', [])),
                'num_circuits': len(advanced_results.get('circuits', {}).get('circuits', [])),
                'output_dir': str(region_output_dir)
            }
            
            logger.info(f"✓ Successfully processed {region_name} in {processing_time:.1f}s")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"✗ Failed to process {region_name}: {e}")
            
            return {
                'region_name': region_name,
                'size': size,
                'status': 'failed',
                'error': str(e),
                'processing_time': processing_time
            }
    
    def _create_segmentation(self, data: np.ndarray) -> np.ndarray:
        """Create segmentation from raw data."""
        # Remove extra dimension if present
        if len(data.shape) == 4:
            data = data.squeeze()
        
        # Use Otsu thresholding
        from skimage import filters
        try:
            threshold = filters.threshold_otsu(data)
        except:
            threshold = np.percentile(data[data > 0], 60)
        
        binary = (data > threshold).astype(np.uint8)
        
        # Label connected components
        from skimage import measure
        labeled = measure.label(binary)
        
        return labeled
    
    def _create_advanced_visualizations(self, volume: np.ndarray, segmentation: np.ndarray,
                                      tracer: NeuronTracer3D, advanced_results: Dict,
                                      output_dir: Path):
        """Create advanced visualizations."""
        try:
            # Create comprehensive visualization
            tracer.create_comprehensive_visualization(str(output_dir))
            
            # Create additional visualizations
            viz = H01Visualizer(str(output_dir))
            if "segmentation" in viz.get_available_datasets():
                viz.create_2d_slice_viewer("segmentation", 
                                         save_path=str(output_dir / "slice_view.png"))
            
            logger.info(f"  ✓ Created visualizations")
            
        except Exception as e:
            logger.warning(f"  ⚠ Visualization failed: {e}")
    
    def _create_advanced_report(self, region_name: str, size: str, volume: np.ndarray,
                              segmentation: np.ndarray, tracer: NeuronTracer3D,
                              advanced_results: Dict, output_dir: Path):
        """Create a comprehensive advanced report."""
        report = {
            'region_name': region_name,
            'size': size,
            'processing_timestamp': datetime.now().isoformat(),
            'data_info': {
                'volume_shape': list(volume.shape),
                'volume_size_mb': volume.nbytes / (1024 * 1024),
                'segmentation_shape': list(segmentation.shape),
                'num_components': int(np.max(segmentation))
            },
            'tracing_results': {
                'num_neurons': len(tracer.traced_neurons),
                'neuron_info': [
                    {
                        'id': int(neuron_id),
                        'volume': int(neuron.volume),
                        'confidence': float(neuron.confidence),
                        'connections': int(len(neuron.connectivity))
                    }
                    for neuron_id, neuron in tracer.traced_neurons.items()
                ]
            },
            'advanced_analysis': advanced_results,
            'system_info': {
                'cpu_count': mp.cpu_count(),
                'memory_gb': psutil.virtual_memory().total / (1024**3),
                'disk_free_gb': psutil.disk_usage('/').free / (1024**3)
            }
        }
        
        report_file = output_dir / "advanced_report.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"  ✓ Created advanced report: {report_file}")
    
    def process_batch(self) -> Dict[str, Any]:
        """Process multiple regions in parallel."""
        logger.info(f"Starting batch processing of {len(self.config.regions)} regions")
        
        results = []
        start_time = time.time()
        
        # Process regions with limited concurrency
        with ProcessPoolExecutor(max_workers=self.config.max_concurrent_regions) as executor:
            # Submit all jobs
            future_to_region = {}
            for region_name in self.config.regions:
                for size in self.config.region_sizes:
                    future = executor.submit(self.process_region_batch, region_name, size)
                    future_to_region[future] = (region_name, size)
            
            # Collect results
            for future in as_completed(future_to_region):
                region_name, size = future_to_region[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    if result['status'] == 'success':
                        self.processed_regions.append(result)
                        logger.info(f"✓ Completed: {region_name} ({size})")
                    else:
                        self.failed_regions.append(result)
                        logger.error(f"✗ Failed: {region_name} ({size})")
                        
                except Exception as e:
                    logger.error(f"✗ Exception in {region_name} ({size}): {e}")
                    self.failed_regions.append({
                        'region_name': region_name,
                        'size': size,
                        'status': 'failed',
                        'error': str(e)
                    })
        
        total_time = time.time() - start_time
        
        # Create batch summary
        summary = self._create_batch_summary(results, total_time)
        
        logger.info(f"Batch processing completed in {total_time:.1f}s")
        logger.info(f"Success: {len(self.processed_regions)}, Failed: {len(self.failed_regions)}")
        
        return summary
    
    def _create_batch_summary(self, results: List[Dict], total_time: float) -> Dict[str, Any]:
        """Create a summary of batch processing."""
        successful = [r for r in results if r['status'] == 'success']
        failed = [r for r in results if r['status'] == 'failed']
        
        summary = {
            'batch_info': {
                'total_regions': len(results),
                'successful': len(successful),
                'failed': len(failed),
                'total_time': total_time,
                'avg_time_per_region': total_time / len(results) if results else 0
            },
            'advanced_stats': {
                'total_neurons': sum(r.get('num_neurons', 0) for r in successful),
                'total_synapses': sum(r.get('num_synapses', 0) for r in successful),
                'total_circuits': sum(r.get('num_circuits', 0) for r in successful)
            },
            'results': results
        }
        
        # Save summary
        summary_file = self.output_dir / "batch_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        logger.info(f"Batch summary saved: {summary_file}")
        return summary

def main():
    """Main function for production batch processing."""
    print("Production H01 Batch Processor")
    print("=" * 50)
    
    # Configuration for large-scale processing
    config = BatchConfig(
        regions=["prefrontal_cortex", "hippocampus", "visual_cortex"],
        region_sizes=["medium", "large"],
        chunk_size=(512, 512, 256),
        overlap=(64, 64, 32),
        max_workers=mp.cpu_count(),
        max_concurrent_regions=2,  # Limit to avoid overwhelming the server
        max_memory_per_worker_gb=8.0,
        total_memory_limit_gb=64.0,
        output_base_dir="h01_production_batch",
        enable_synapse_detection=True,
        enable_circuit_mapping=True,
        enable_swc_export=True,
        enable_neurolucida_export=True
    )
    
    # Initialize processor
    processor = ProductionBatchProcessor(config)
    
    print(f"Processing regions: {config.regions}")
    print(f"Region sizes: {config.region_sizes}")
    print(f"Output directory: {config.output_base_dir}")
    print(f"Advanced analysis: Synapse detection, Circuit mapping, SWC/Neurolucida export")
    
    try:
        # Process batch
        summary = processor.process_batch()
        
        # Print results
        print(f"\n{'='*50}")
        print(f"BATCH PROCESSING SUMMARY")
        print(f"{'='*50}")
        
        batch_info = summary['batch_info']
        print(f"Total regions: {batch_info['total_regions']}")
        print(f"Successful: {batch_info['successful']}")
        print(f"Failed: {batch_info['failed']}")
        print(f"Total time: {batch_info['total_time']:.1f}s")
        print(f"Average time per region: {batch_info['avg_time_per_region']:.1f}s")
        
        advanced_stats = summary['advanced_stats']
        print(f"\nAdvanced Analysis Results:")
        print(f"  Total neurons: {advanced_stats['total_neurons']}")
        print(f"  Total synapses: {advanced_stats['total_synapses']}")
        print(f"  Total circuits: {advanced_stats['total_circuits']}")
        
        print(f"\nResults saved in: {config.output_base_dir}")
        
    except Exception as e:
        logger.error(f"Batch processing failed: {e}")
        raise

if __name__ == "__main__":
    main() 