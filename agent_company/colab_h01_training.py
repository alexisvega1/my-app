#!/usr/bin/env python3
"""
H01 FFN-v2 Training in Google Colab
===================================
This script trains the MathematicalFFNv2 model on real H01 connectomics data,
leveraging the H01DataLoader and a YAML configuration.
"""

# ============================================================================
# SETUP AND INSTALLATION
# ============================================================================

print("üöÄ Setting up H01 Training Environment...")
!pip install torch torchvision matplotlib numpy scipy tqdm cloud-volume PyYAML -q

# Import Libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import matplotlib.pyplot as plt
import time
from tqdm import tqdm
import warnings
import os
warnings.filterwarnings('ignore')

# Import project-specific modules
# This assumes the script is run from the 'agent_company' directory
from h01_data_loader import H01DataLoader
from ffn_v2_mathematical_model import MathematicalFFNv2
import yaml

# ============================================================================
# GPU AND PYTORCH CONFIGURATION
# ============================================================================

print("\nüîç Configuring GPU and PyTorch...")
if torch.cuda.is_available():
    print("‚úÖ GPU available. Setting device to 'cuda'.")
    device = torch.device('cuda')
    # Optimizations for A100 GPU
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
else:
    print("‚ö†Ô∏è GPU not available. Using CPU.")
    device = torch.device('cpu')

print(f"   - Using device: {device}")
print(f"   - PyTorch version: {torch.__version__}")

# ============================================================================
# H01 DATASET CLASS FOR PYTORCH
# ============================================================================

class H01Dataset(Dataset):
    """PyTorch Dataset wrapper for the H01DataLoader."""
    def __init__(self, data_loader: H01DataLoader, region_name: str, samples: int = 100, chunk_size: tuple = (64, 64, 64)):
        self.loader = data_loader
        self.region = self.loader.get_region(region_name)
        self.bounds = self.region['bounds']
        self.samples = samples
        self.chunk_size = chunk_size
        
        # Pre-generate random coordinates for fetching chunks
        self.coordinates = self._generate_coordinates()
        print(f"‚úÖ H01 Dataset initialized for region '{region_name}' with {samples} samples.")

    def _generate_coordinates(self):
        z_min, y_min, x_min = self.bounds[0]
        z_max, y_max, x_max = self.bounds[1]
        
        # Ensure we don't sample outside the bounds when considering chunk size
        z_range = z_max - z_min - self.chunk_size[0]
        y_range = y_max - y_min - self.chunk_size[1]
        x_range = x_max - x_min - self.chunk_size[2]

        coords = []
        for _ in range(self.samples):
            z = np.random.randint(0, z_range) + z_min
            y = np.random.randint(0, y_range) + y_min
            x = np.random.randint(0, x_range) + x_min
            coords.append((z, y, x))
        return coords

    def __len__(self):
        return self.samples

    def __getitem__(self, idx):
        coords = self.coordinates[idx]
        # Load a chunk of data (this will be our input)
        data_chunk = self.loader.load_chunk(coords, self.chunk_size)
        
        # Normalize and convert to tensor
        input_tensor = torch.from_numpy(data_chunk.astype(np.float32)).unsqueeze(0)
        
        # Basic normalization
        mean = input_tensor.mean()
        std = input_tensor.std()
        if std > 0:
            input_tensor = (input_tensor - mean) / std

        # For this example, we'll create a dummy target.
        # In a real scenario, you would load a corresponding segmentation label chunk.
        target_tensor = torch.randint(0, 2, input_tensor.shape, dtype=torch.float32)
        
        return input_tensor, target_tensor

# ============================================================================
# TRAINING LOOP
# ============================================================================

def train(model, device, train_loader, epochs=5):
    """A simplified training loop."""
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    # Use BCEWithLogitsLoss because our model's output is a raw logit (no sigmoid)
    # But our model DOES have a sigmoid, so we must use BCELoss.
    # Let's stick with the original loss function for now.
    loss_fn = nn.BCELoss()
    
    print("\nüöÄ Starting H01 Training...")
    model.train()
    for epoch in range(epochs):
        epoch_loss = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}", leave=False)
        for inputs, targets in progress_bar:
            inputs, targets = inputs.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_fn(outputs, targets)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})

        avg_loss = epoch_loss / len(train_loader)
        print(f"‚úÖ Epoch {epoch+1} complete. Average Loss: {avg_loss:.4f}")

    print("üéâ Training finished.")
    return model

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    print("\nüß† Starting H01 FFN-v2 Training script.")
    
    # --- Configuration ---
    config_path = 'h01_config.yaml'
    train_region = 'test_region_1'
    num_samples = 50
    batch_size = 4
    epochs = 3
    chunk_size = (64, 64, 64)

    # 1. Load H01 Data Loader
    if not os.path.exists(config_path):
        print(f"‚ùå Config file not found at {config_path}")
        print("   Please ensure you are running this from the `agent_company` directory.")
        return

    print(f"\n1. Loading H01 data loader with config: {config_path}")
    try:
        with open(config_path, 'r') as f:
            h01_config = yaml.safe_load(f)
        data_loader = H01DataLoader(h01_config)
        print("‚úÖ H01 data loader created successfully.")
    except Exception as e:
        print(f"‚ùå Failed to load H01 data loader: {e}")
        return

    # 2. Create PyTorch Dataset and DataLoader
    print(f"\n2. Creating PyTorch dataset for region '{train_region}'...")
    h01_dataset = H01Dataset(data_loader, train_region, samples=num_samples, chunk_size=chunk_size)
    # Use num_workers > 0 to fetch data in parallel to GPU training
    train_loader = DataLoader(h01_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
    print("‚úÖ DataLoader ready.")

    # 3. Initialize Model
    print("\n3. Initializing MathematicalFFNv2 model...")
    model = MathematicalFFNv2(
        input_channels=1,
        output_channels=1,
        hidden_channels=64,
        depth=3
    )
    print("‚úÖ Model initialized.")

    # 4. Run Training
    trained_model = train(model, device, train_loader, epochs=epochs)

    # 5. Save the trained model
    model_save_path = 'h01_trained_model.pt'
    print(f"\n5. Saving trained model to {model_save_path}...")
    torch.save(trained_model.state_dict(), model_save_path)
    print("‚úÖ Model saved.")

    print("\nüèÅ H01 training script finished successfully!")
    print(f"   To use this model, load '{model_save_path}' in your production pipeline.")

if __name__ == "__main__":
    main()