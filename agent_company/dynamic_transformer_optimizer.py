#!/usr/bin/env python3
"""
Dynamic Transformer Hyperparameter Optimizer for Connectomics
============================================================

This system provides dynamic, adaptive hyperparameter optimization for transformer
architectures used in connectomics applications.
"""

import asyncio
import time
import json
import logging
import random
import math
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TransformerConfig:
    """Dynamic transformer configuration"""
    # Architecture parameters
    model_type: str = "vit"
    input_channels: int = 1
    hidden_size: int = 768
    num_layers: int = 12
    num_heads: int = 12
    intermediate_size: int = 3072
    dropout_rate: float = 0.1
    attention_dropout: float = 0.1
    
    # Dynamic learning parameters
    learning_rate: float = 1e-4
    min_lr: float = 1e-6
    max_lr: float = 1e-2
    lr_schedule: str = "cosine"
    warmup_steps: int = 1000
    weight_decay: float = 0.01
    
    # Batch and optimization parameters
    batch_size: int = 32
    min_batch_size: int = 8
    max_batch_size: int = 128
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1.0
    
    # Training dynamics
    patience: int = 10
    min_patience: int = 3
    max_patience: int = 50
    early_stopping_threshold: float = 0.001
    
    # Performance thresholds
    target_accuracy: float = 0.95
    target_loss: float = 0.1
    memory_threshold_gb: float = 32.0
    gpu_utilization_threshold: float = 0.9

@dataclass
class OptimizationMetrics:
    """Metrics for hyperparameter optimization"""
    loss: float
    accuracy: float
    learning_rate: float
    batch_size: int
    memory_usage_gb: float
    gpu_utilization: float
    training_time_per_step: float
    convergence_rate: float
    gradient_norm: float
    learning_efficiency: float
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class OptimizationResult:
    """Results from hyperparameter optimization"""
    best_config: TransformerConfig
    best_metrics: OptimizationMetrics
    optimization_history: List[OptimizationMetrics]
    convergence_analysis: Dict[str, Any]
    recommendations: List[str]
    optimization_time: float

class DynamicLearningRateScheduler:
    """Dynamic learning rate scheduler with adaptive adjustment"""
    
    def __init__(self, initial_lr: float, config: TransformerConfig):
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.config = config
        self.step_count = 0
        self.loss_history = []
        self.lr_history = []
        
        # Adaptive parameters
        self.patience_counter = 0
        self.best_loss = float('inf')
        self.lr_reduction_factor = 0.5
        self.lr_increase_factor = 1.1
        self.min_lr_improvement = 0.001
        
    def step(self, loss: float, gradient_norm: float = None) -> float:
        """Update learning rate based on training dynamics"""
        self.step_count += 1
        self.loss_history.append(loss)
        self.lr_history.append(self.current_lr)
        
        # Adaptive learning rate adjustment
        if len(self.loss_history) > 1:
            loss_improvement = self.best_loss - loss
            
            if loss < self.best_loss:
                self.best_loss = loss
                self.patience_counter = 0
                
                # Increase LR if improving rapidly
                if loss_improvement > self.min_lr_improvement * 2:
                    self.current_lr = min(
                        self.current_lr * self.lr_increase_factor,
                        self.config.max_lr
                    )
            else:
                self.patience_counter += 1
                
                # Reduce LR if not improving
                if self.patience_counter >= self.config.patience:
                    self.current_lr = max(
                        self.current_lr * self.lr_reduction_factor,
                        self.config.min_lr
                    )
                    self.patience_counter = 0
        
        # Gradient-based adjustment
        if gradient_norm is not None:
            if gradient_norm > self.config.max_grad_norm * 1.5:
                # Reduce LR if gradients are too large
                self.current_lr *= 0.9
            elif gradient_norm < self.config.max_grad_norm * 0.5:
                # Increase LR if gradients are too small
                self.current_lr *= 1.05
        
        # Apply warmup
        if self.step_count < self.config.warmup_steps:
            warmup_factor = self.step_count / self.config.warmup_steps
            self.current_lr = self.initial_lr * warmup_factor
        
        return self.current_lr
    
    def get_lr(self) -> float:
        """Get current learning rate"""
        return self.current_lr
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get scheduler metrics"""
        return {
            'current_lr': self.current_lr,
            'step_count': self.step_count,
            'best_loss': self.best_loss,
            'patience_counter': self.patience_counter,
            'loss_history': self.loss_history[-100:],
            'lr_history': self.lr_history[-100:]
        }

class AdaptiveBatchSizeOptimizer:
    """Dynamic batch size optimization based on memory and performance"""
    
    def __init__(self, initial_batch_size: int, config: TransformerConfig):
        self.current_batch_size = initial_batch_size
        self.config = config
        self.memory_history = []
        self.performance_history = []
        self.optimal_batch_size = initial_batch_size
        
    def update_batch_size(self, memory_usage_gb: float, gpu_utilization: float, 
                         training_time_per_step: float) -> int:
        """Update batch size based on system metrics"""
        self.memory_history.append(memory_usage_gb)
        self.performance_history.append(training_time_per_step)
        
        # Memory-based adjustment
        if memory_usage_gb > self.config.memory_threshold_gb * 0.9:
            # Reduce batch size if memory usage is high
            self.current_batch_size = max(
                self.current_batch_size // 2,
                self.config.min_batch_size
            )
        elif memory_usage_gb < self.config.memory_threshold_gb * 0.6:
            # Increase batch size if memory usage is low
            self.current_batch_size = min(
                self.current_batch_size * 2,
                self.config.max_batch_size
            )
        
        # GPU utilization-based adjustment
        if gpu_utilization < 0.7:
            # Increase batch size if GPU is underutilized
            self.current_batch_size = min(
                self.current_batch_size + 8,
                self.config.max_batch_size
            )
        elif gpu_utilization > 0.95:
            # Reduce batch size if GPU is overutilized
            self.current_batch_size = max(
                self.current_batch_size - 4,
                self.config.min_batch_size
            )
        
        # Performance-based adjustment
        if len(self.performance_history) > 5:
            recent_performance = sum(self.performance_history[-5:]) / 5
            if len(self.performance_history) >= 10:
                previous_performance = sum(self.performance_history[-10:-5]) / 5
                if recent_performance > previous_performance * 1.2:
                    # Reduce batch size if performance is degrading
                    self.current_batch_size = max(
                        self.current_batch_size - 2,
                        self.config.min_batch_size
                    )
        
        return self.current_batch_size
    
    def get_optimal_batch_size(self) -> int:
        """Get the optimal batch size based on historical data"""
        if len(self.memory_history) > 10:
            # Find batch size that maximizes memory efficiency
            memory_efficiency = []
            for i in range(len(self.memory_history) - 1):
                if self.memory_history[i] > 0:
                    efficiency = self.performance_history[i] / self.memory_history[i]
                    memory_efficiency.append(efficiency)
            
            if memory_efficiency:
                optimal_idx = memory_efficiency.index(min(memory_efficiency))
                return self.current_batch_size
        
        return self.current_batch_size

class SimpleHyperparameterOptimizer:
    """Simple hyperparameter optimizer without external dependencies"""
    
    def __init__(self, config: TransformerConfig):
        self.config = config
        self.optimization_history = []
        
    def optimize(self, n_trials: int = 50) -> OptimizationResult:
        """Run simple hyperparameter optimization"""
        start_time = time.time()
        
        # Define parameter ranges
        learning_rates = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]
        batch_sizes = [8, 16, 32, 64, 128]
        hidden_sizes = [512, 768, 1024, 1536]
        num_layers = [6, 12, 18, 24]
        num_heads = [8, 12, 16, 24]
        dropout_rates = [0.0, 0.1, 0.2, 0.3]
        weight_decays = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]
        
        best_loss = float('inf')
        best_config = self.config
        best_metrics = None
        
        # Grid search over parameter combinations
        for lr in learning_rates:
            for batch_size in batch_sizes:
                for hidden_size in hidden_sizes:
                    for num_layer in num_layers:
                        for num_head in num_heads:
                            for dropout_rate in dropout_rates:
                                for weight_decay in weight_decays:
                                    # Simulate training with these parameters
                                    simulated_loss = self._simulate_training(
                                        lr, batch_size, hidden_size, num_layer, 
                                        num_head, dropout_rate, weight_decay
                                    )
                                    
                                    if simulated_loss < best_loss:
                                        best_loss = simulated_loss
                                        best_config = TransformerConfig(
                                            learning_rate=lr,
                                            batch_size=batch_size,
                                            hidden_size=hidden_size,
                                            num_layers=num_layer,
                                            num_heads=num_head,
                                            dropout_rate=dropout_rate,
                                            weight_decay=weight_decay
                                        )
                                        
                                        best_metrics = OptimizationMetrics(
                                            loss=simulated_loss,
                                            accuracy=1.0 - simulated_loss,
                                            learning_rate=lr,
                                            batch_size=batch_size,
                                            memory_usage_gb=8.0,
                                            gpu_utilization=0.8,
                                            training_time_per_step=0.1,
                                            convergence_rate=0.95,
                                            gradient_norm=1.0,
                                            learning_efficiency=0.9
                                        )
        
        optimization_time = time.time() - start_time
        
        return OptimizationResult(
            best_config=best_config,
            best_metrics=best_metrics,
            optimization_history=[],
            convergence_analysis=self._analyze_convergence(),
            recommendations=self._generate_recommendations(best_config),
            optimization_time=optimization_time
        )
    
    def _simulate_training(self, lr, batch_size, hidden_size, num_layers, 
                          num_heads, dropout_rate, weight_decay) -> float:
        """Simulate training to estimate validation loss"""
        # Simplified simulation based on hyperparameter relationships
        base_loss = 0.5
        
        # Adjust loss based on hyperparameters
        lr_factor = 1.0 / (1.0 + lr * 1000)  # Lower LR = better
        batch_factor = 1.0 / (1.0 + batch_size / 64)  # Larger batch = better
        size_factor = 1.0 / (1.0 + hidden_size / 1024)  # Larger model = better
        layer_factor = 1.0 / (1.0 + num_layers / 12)  # More layers = better
        dropout_factor = 1.0 + dropout_rate * 0.5  # Higher dropout = slightly worse
        weight_decay_factor = 1.0 + weight_decay * 2  # Higher weight decay = slightly worse
        
        # Add some randomness
        noise = random.uniform(-0.1, 0.1)
        
        simulated_loss = (base_loss * lr_factor * batch_factor * size_factor * 
                         layer_factor * dropout_factor * weight_decay_factor + noise)
        return max(0.01, simulated_loss)
    
    def _analyze_convergence(self) -> Dict[str, Any]:
        """Analyze optimization convergence"""
        return {
            'total_trials': 1000,  # Estimated
            'successful_trials': 1000,
            'best_value': 0.1,  # Estimated
            'worst_value': 0.8,  # Estimated
            'mean_value': 0.4,  # Estimated
            'std_value': 0.2,  # Estimated
            'convergence_rate': 0.85  # Estimated
        }
    
    def _generate_recommendations(self, best_config: TransformerConfig) -> List[str]:
        """Generate recommendations based on optimization results"""
        recommendations = []
        
        if best_config.learning_rate < 1e-4:
            recommendations.append("Consider using a higher learning rate for faster convergence")
        elif best_config.learning_rate > 1e-3:
            recommendations.append("Lower learning rate may improve stability")
        
        if best_config.batch_size < 32:
            recommendations.append("Larger batch size may improve training efficiency")
        elif best_config.batch_size > 64:
            recommendations.append("Smaller batch size may improve generalization")
        
        if best_config.hidden_size < 768:
            recommendations.append("Larger hidden size may improve model capacity")
        elif best_config.hidden_size > 1024:
            recommendations.append("Smaller hidden size may reduce overfitting")
        
        if best_config.num_layers < 12:
            recommendations.append("More layers may improve model expressiveness")
        elif best_config.num_layers > 18:
            recommendations.append("Fewer layers may reduce computational cost")
        
        return recommendations

class DynamicTransformerOptimizer:
    """Main dynamic transformer optimizer"""
    
    def __init__(self, initial_config: TransformerConfig):
        self.config = initial_config
        self.lr_scheduler = DynamicLearningRateScheduler(initial_config.learning_rate, initial_config)
        self.batch_optimizer = AdaptiveBatchSizeOptimizer(initial_config.batch_size, initial_config)
        self.hyperparameter_optimizer = SimpleHyperparameterOptimizer(initial_config)
        
        self.optimization_history = []
        self.current_metrics = None
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def optimize_hyperparameters(self, training_data: Any = None, 
                                     validation_data: Any = None,
                                     max_iterations: int = 100) -> OptimizationResult:
        """Main optimization loop"""
        self.logger.info("Starting dynamic hyperparameter optimization...")
        
        start_time = time.time()
        
        # Phase 1: Initial hyperparameter optimization
        self.logger.info("Phase 1: Initial hyperparameter optimization")
        initial_result = self.hyperparameter_optimizer.optimize(n_trials=50)
        
        # Update config with optimized parameters
        self.config = initial_result.best_config
        self.lr_scheduler = DynamicLearningRateScheduler(self.config.learning_rate, self.config)
        self.batch_optimizer = AdaptiveBatchSizeOptimizer(self.config.batch_size, self.config)
        
        # Phase 2: Dynamic optimization during training
        self.logger.info("Phase 2: Dynamic optimization during training")
        for iteration in range(max_iterations):
            # Simulate training step
            metrics = await self._simulate_training_step(iteration)
            
            # Update learning rate
            new_lr = self.lr_scheduler.step(metrics.loss, metrics.gradient_norm)
            
            # Update batch size
            new_batch_size = self.batch_optimizer.update_batch_size(
                metrics.memory_usage_gb,
                metrics.gpu_utilization,
                metrics.training_time_per_step
            )
            
            # Update config
            self.config.learning_rate = new_lr
            self.config.batch_size = new_batch_size
            
            # Store metrics
            self.optimization_history.append(metrics)
            
            # Check convergence
            if self._check_convergence(metrics):
                self.logger.info(f"Converged at iteration {iteration}")
                break
            
            # Log progress
            if iteration % 10 == 0:
                self.logger.info(f"Iteration {iteration}: Loss={metrics.loss:.4f}, "
                               f"LR={new_lr:.2e}, Batch={new_batch_size}")
        
        optimization_time = time.time() - start_time
        
        # Generate final result
        final_metrics = self.optimization_history[-1] if self.optimization_history else initial_result.best_metrics
        
        result = OptimizationResult(
            best_config=self.config,
            best_metrics=final_metrics,
            optimization_history=self.optimization_history,
            convergence_analysis=self._analyze_convergence(),
            recommendations=self._generate_final_recommendations(),
            optimization_time=optimization_time
        )
        
        self.logger.info(f"Optimization completed in {optimization_time:.2f} seconds")
        return result
    
    async def _simulate_training_step(self, iteration: int) -> OptimizationMetrics:
        """Simulate a training step to get metrics"""
        # Simulate training dynamics
        base_loss = 0.5 * math.exp(-iteration / 50)  # Decreasing loss
        noise = random.uniform(-0.05, 0.05)
        loss = max(0.01, base_loss + noise)
        
        # Simulate accuracy improvement
        accuracy = min(0.99, 0.5 + 0.4 * (1 - math.exp(-iteration / 30)))
        
        # Simulate memory usage
        memory_usage = 8.0 + 2.0 * math.sin(iteration / 10) + random.uniform(-0.5, 0.5)
        
        # Simulate GPU utilization
        gpu_utilization = 0.7 + 0.2 * math.sin(iteration / 15) + random.uniform(-0.1, 0.1)
        
        # Simulate training time
        training_time = 0.1 + 0.05 * math.sin(iteration / 20) + random.uniform(-0.01, 0.01)
        
        # Simulate gradient norm
        gradient_norm = 1.0 + 0.5 * math.sin(iteration / 25) + random.uniform(-0.1, 0.1)
        
        return OptimizationMetrics(
            loss=loss,
            accuracy=accuracy,
            learning_rate=self.config.learning_rate,
            batch_size=self.config.batch_size,
            memory_usage_gb=memory_usage,
            gpu_utilization=gpu_utilization,
            training_time_per_step=training_time,
            convergence_rate=0.95 if iteration > 50 else 0.5,
            gradient_norm=gradient_norm,
            learning_efficiency=0.9 if iteration > 30 else 0.6
        )
    
    def _check_convergence(self, metrics: OptimizationMetrics) -> bool:
        """Check if optimization has converged"""
        if len(self.optimization_history) < 10:
            return False
        
        # Check loss convergence
        recent_losses = [m.loss for m in self.optimization_history[-10:]]
        loss_mean = sum(recent_losses) / len(recent_losses)
        loss_variance = sum((x - loss_mean) ** 2 for x in recent_losses) / len(recent_losses)
        loss_std = math.sqrt(loss_variance)
        
        # Check accuracy convergence
        recent_accuracies = [m.accuracy for m in self.optimization_history[-10:]]
        accuracy_improvement = recent_accuracies[-1] - recent_accuracies[0]
        
        # Check learning rate stability
        recent_lrs = [m.learning_rate for m in self.optimization_history[-10:]]
        lr_change = abs(recent_lrs[-1] - recent_lrs[0]) / recent_lrs[0]
        
        return (loss_std < 0.01 and 
                accuracy_improvement < 0.01 and 
                lr_change < 0.1)
    
    def _analyze_convergence(self) -> Dict[str, Any]:
        """Analyze optimization convergence"""
        if not self.optimization_history:
            return {}
        
        losses = [m.loss for m in self.optimization_history]
        accuracies = [m.accuracy for m in self.optimization_history]
        learning_rates = [m.learning_rate for m in self.optimization_history]
        
        return {
            'total_iterations': len(self.optimization_history),
            'final_loss': losses[-1],
            'final_accuracy': accuracies[-1],
            'loss_improvement': losses[0] - losses[-1],
            'accuracy_improvement': accuracies[-1] - accuracies[0],
            'lr_final': learning_rates[-1],
            'lr_change': abs(learning_rates[-1] - learning_rates[0]) / learning_rates[0],
            'convergence_rate': self._calculate_convergence_rate(losses)
        }
    
    def _calculate_convergence_rate(self, losses: List[float]) -> float:
        """Calculate convergence rate based on loss improvement"""
        if len(losses) < 10:
            return 0.0
        
        initial_loss = sum(losses[:5]) / 5
        final_loss = sum(losses[-5:]) / 5
        
        if initial_loss == 0:
            return 0.0
        
        improvement = (initial_loss - final_loss) / initial_loss
        return max(0.0, improvement)
    
    def _generate_final_recommendations(self) -> List[str]:
        """Generate final recommendations"""
        recommendations = []
        
        if not self.optimization_history:
            return ["No optimization history available"]
        
        final_metrics = self.optimization_history[-1]
        
        # Learning rate recommendations
        if final_metrics.learning_rate < 1e-5:
            recommendations.append("Learning rate is very low - consider increasing for faster convergence")
        elif final_metrics.learning_rate > 1e-3:
            recommendations.append("Learning rate is high - consider reducing for better stability")
        
        # Batch size recommendations
        if final_metrics.batch_size < 16:
            recommendations.append("Small batch size detected - consider increasing for better GPU utilization")
        elif final_metrics.batch_size > 64:
            recommendations.append("Large batch size detected - consider reducing for better generalization")
        
        # Memory recommendations
        if final_metrics.memory_usage_gb > self.config.memory_threshold_gb * 0.8:
            recommendations.append("High memory usage - consider reducing model size or batch size")
        
        # GPU utilization recommendations
        if final_metrics.gpu_utilization < 0.7:
            recommendations.append("Low GPU utilization - consider increasing batch size or model complexity")
        
        # Convergence recommendations
        convergence_analysis = self._analyze_convergence()
        if convergence_analysis.get('convergence_rate', 0) < 0.5:
            recommendations.append("Low convergence rate - consider adjusting learning rate schedule")
        
        return recommendations

async def main():
    """Main function to demonstrate dynamic transformer optimization"""
    print("ðŸš€ Starting Dynamic Transformer Hyperparameter Optimization...")
    
    # Initialize with default config
    initial_config = TransformerConfig(
        learning_rate=1e-4,
        batch_size=32,
        hidden_size=768,
        num_layers=12,
        num_heads=12,
        dropout_rate=0.1,
        weight_decay=0.01
    )
    
    # Create optimizer
    optimizer = DynamicTransformerOptimizer(initial_config)
    
    # Run optimization
    result = await optimizer.optimize_hyperparameters(max_iterations=100)
    
    # Print results
    print(f"\nðŸŽ¯ Optimization Results:")
    print(f"   Final Loss: {result.best_metrics.loss:.4f}")
    print(f"   Final Accuracy: {result.best_metrics.accuracy:.4f}")
    print(f"   Final Learning Rate: {result.best_metrics.learning_rate:.2e}")
    print(f"   Final Batch Size: {result.best_metrics.batch_size}")
    print(f"   Optimization Time: {result.optimization_time:.2f}s")
    print(f"   Total Iterations: {len(result.optimization_history)}")
    
    print(f"\nðŸ“Š Convergence Analysis:")
    for key, value in result.convergence_analysis.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.4f}")
        else:
            print(f"   {key}: {value}")
    
    print(f"\nðŸ’¡ Recommendations:")
    for i, rec in enumerate(result.recommendations, 1):
        print(f"   {i}. {rec}")
    
    # Save results
    with open('transformer_optimization_results.json', 'w') as f:
        json.dump({
            'best_config': {
                'learning_rate': result.best_config.learning_rate,
                'batch_size': result.best_config.batch_size,
                'hidden_size': result.best_config.hidden_size,
                'num_layers': result.best_config.num_layers,
                'num_heads': result.best_config.num_heads,
                'dropout_rate': result.best_config.dropout_rate,
                'weight_decay': result.best_config.weight_decay
            },
            'best_metrics': {
                'loss': result.best_metrics.loss,
                'accuracy': result.best_metrics.accuracy,
                'learning_rate': result.best_metrics.learning_rate,
                'batch_size': result.best_metrics.batch_size,
                'memory_usage_gb': result.best_metrics.memory_usage_gb,
                'gpu_utilization': result.best_metrics.gpu_utilization
            },
            'convergence_analysis': result.convergence_analysis,
            'recommendations': result.recommendations,
            'optimization_time': result.optimization_time
        }, f, indent=2)
    
    print(f"\nðŸ“„ Results saved to: transformer_optimization_results.json")

if __name__ == "__main__":
    asyncio.run(main())
